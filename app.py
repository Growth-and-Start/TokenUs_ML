from flask import Flask, request, jsonify
import os
import requests
import boto3
from dotenv import load_dotenv
import cv2
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50
import faiss
import numpy as np
import pymysql

import logging

app = Flask(__name__)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(".env")

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_DEFAULT_REGION")
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ env íŒŒì¼ í˜¸ì¶œë¨!")

if not all([AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, S3_BUCKET_NAME]):
    raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# DB ì—°ê²° í•¨ìˆ˜
def get_db_connection():
    return pymysql.connect(
        host=os.getenv("MYSQL_HOST"),
        port=int(os.getenv("MYSQL_PORT")),
        user=os.getenv("MYSQL_USER"),
        password=os.getenv("MYSQL_PASSWORD"),
        database=os.getenv("MYSQL_DATABASE"),
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )
logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ DBì—°ê²°ë¨!")

# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±ë¨!")

# ë‹¤ìš´ë¡œë“œëœ ì˜ìƒì„ ì €ì¥í•  í´ë” ìƒì„±
DOWNLOAD_FOLDER = "downloads"
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)
logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ ë‹¤ìš´ë¡œë“œí•  í´ë” ì¤€ë¹„ ì™„ë£Œ!")

def delete_s3_file(s3_url):
    """
    S3ì— ì—…ë¡œë“œëœ ì˜ìƒì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # S3 URLì—ì„œ ë²„í‚·ëª…ê³¼ íŒŒì¼ í‚¤ ì¶”ì¶œ
        bucket_name = "tokenus-storage"  # âœ… S3 ë²„í‚·ëª…
        key = s3_url.replace(f"https://{bucket_name}.s3.ap-northeast-2.amazonaws.com/", "")

        # S3 ê°ì²´ ì‚­ì œ
        s3_client.delete_object(Bucket=bucket_name, Key=key)
        logger.info(f"âœ… S3ì—ì„œ íŒŒì¼ ì‚­ì œ ì„±ê³µ: {s3_url}")

    except Exception as e:
        logger.info(f"âŒ S3ì—ì„œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {s3_url}, ì˜¤ë¥˜: {e}")

@app.route("/download", methods=['POST'])
def download_video():
    logger.info("ğŸ”¥ğŸ”¥ğŸ”¥ download_video í˜¸ì¶œë¨!")
    data = request.json
    video_url = data.get('file_url')
    logger.info(f"ğŸ”¥ì˜ìƒ url:{video_url}")
    
    if not video_url:
        return jsonify({"error": "No video_url provided"}), 400
    
    # S3 ê°ì²´ í‚¤(íŒŒì¼ ê²½ë¡œ) ì¶”ì¶œ
    object_key = video_url.split(".com/")[-1]  # "videos/test3.mov"
    
    try:
        # ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
        filename = object_key.split("/")[-1]
        file_path = os.path.join(DOWNLOAD_FOLDER, filename)
        
        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        s3_client.download_file(S3_BUCKET_NAME, object_key, file_path)
        
        #return jsonify({"message": "Download successful", "file_path": file_path})
        
        # âœ… ìë™ìœ¼ë¡œ check_similarity ìˆ˜í–‰
        video_id = get_next_video_id()
        similarity_result = perform_similarity_check(file_path, video_id, video_url)

        return jsonify({
            "message": "Download successful",
            "video_url": video_url,
            "similarity_check_result": similarity_result
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500
#object_key : ì˜ìƒì˜ S3 url

def perform_similarity_check(video_path, video_id, video_url):
    """
    ì €ì¥ëœ ëª¨ë“  ë²¡í„°ì™€ ì…ë ¥ëœ ì˜ìƒì˜ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ.
    ê°™ì€ ì˜ìƒì¸ì§€ íŒë‹¨í•˜ëŠ” ê¸°ì¤€:
    - í•œ ê°œ ì´ìƒì˜ ë²¡í„°ê°€ 0.8 ì´ìƒ
    - í‰ê·  ìœ ì‚¬ë„ê°€ 0.75 ì´ìƒ
    """
    logger.info("ğŸ”¥SimilarityCheckì‹œì‘")
    if not video_path or not os.path.exists(video_path):
        return {"error": "Invalid video path"}

    try:
        # 1ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        load_faiss_index()

        # 2ï¸âƒ£ ì €ì¥ëœ ë²¡í„° ê°œìˆ˜ í™•ì¸
        total_vectors = faiss_index.index.ntotal
        
        # 0ï¸âƒ£ì €ì¥ëœ ë¹„ë””ì˜¤ê°€ ì—†ì„ë•Œ
        if total_vectors == 0:
            # ğŸ”¹ ë¹„êµí•  ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ë° ë²¡í„°í™”
            frames = extract_frames(video_path)
            feature_vectors = extract_features(frames)

            # ğŸ”¹ FAISS ë° DB ì €ì¥
            start_index = faiss_index.index.ntotal
            faiss_index.add_vectors(feature_vectors)
            save_faiss_index()
            insert_vector_metadata(video_id, start_index, len(feature_vectors))

            similarity_result = {
                "message": "ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µê³¼í•˜ì˜€ìŠµë‹ˆë‹¤",
                "max_similarity": 0,
                "avg_similarity": 0,
                "passed":True,
                "video_url":video_url
            }
            logger.info(similarity_result)

            notify_springboot(similarity_result)
            delete_file(video_path)
            
            return similarity_result
            

        # 3ï¸âƒ£ ë¹„êµí•  ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
        frames = extract_frames(video_path)

        # 4ï¸âƒ£ í”„ë ˆì„ì˜ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
        feature_vectors = extract_features(frames)

        # 5ï¸âƒ£ ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¹„êµ)
        query_vectors = np.array(feature_vectors).astype('float32')
        faiss.normalize_L2(query_vectors)

        # 6ï¸âƒ£ ê° í”„ë ˆì„ì˜ ë²¡í„°ë¥¼ FAISSì— ì €ì¥ëœ ëª¨ë“  ë²¡í„°ì™€ ë¹„êµ
        similarity_scores = []
        for query_vector in query_vectors:
            query_vector = query_vector.reshape(1, -1)
            faiss.normalize_L2(query_vector)  # âœ… ê²€ìƒ‰ ë²¡í„° ì •ê·œí™”
            distances, _ = faiss_index.index.search(query_vector, total_vectors)
            similarity_scores.extend(distances[0].tolist())

        # 7ï¸âƒ£ ê²€ì‚¬ ê¸°ì¤€ ì ìš©
        max_similarity = max(similarity_scores)
        avg_similarity = sum(similarity_scores) / len(similarity_scores)

        similarity_result = {
            "max_similarity": max_similarity,
            "avg_similarity": avg_similarity,
            "video_url":video_url
        }
        logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")

        if max_similarity >= 1.0 or (max_similarity >= 0.9 and avg_similarity >= 0.8):
            similarity_result["message"] = "ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤"
            logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")
            similarity_result["passed"]=False
            logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")
            
            # ğŸ”¥ ê°€ì¥ ìœ ì‚¬í•œ ì¸ë±ìŠ¤ ì°¾ê¸°
            query_vector = query_vectors[0].reshape(1, -1)
            faiss.normalize_L2(query_vector)
            distances, indices = faiss_index.index.search(query_vector, total_vectors)
            most_similar_idx = indices[0][0]
            logger.info(f"ğŸ” ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„° ì¸ë±ìŠ¤: {most_similar_idx}")

            # ğŸ”¥ í•´ë‹¹ ë²¡í„°ì˜ video_id ì¡°íšŒ
            similar_video_id = get_video_id_by_faiss_index(most_similar_idx)
            logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")
            similarity_result["similar_video_id"] = similar_video_id
            logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")
            
            # âŒ ë¡œì»¬ íŒŒì¼ ì‚­ì œ
            delete_file(video_path)

            # âŒ S3ì—ì„œë„ ì‚­ì œ
            delete_s3_file(video_path)

            # âŒ Spring Boot ì„œë²„ì— ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨ ì•Œë¦¼
            notify_springboot(similarity_result)
            return similarity_result
        else:
            similarity_result["message"] = "ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µê³¼í•˜ì˜€ìŠµë‹ˆë‹¤"
            similarity_result["passed"]=True

            # ğŸ”¹ FAISS + MySQL ì €ì¥
            start_index = faiss_index.index.ntotal
            faiss_index.add_vectors(feature_vectors)
            save_faiss_index()
            insert_vector_metadata(video_id, start_index, len(feature_vectors))

            # âŒ ë¡œì»¬ íŒŒì¼ ì‚­ì œ
            delete_file(video_path)

            # âœ… Spring Boot ì„œë²„ì— ìœ ì‚¬ë„ ê²€ì‚¬ ì„±ê³µ ì•Œë¦¼
            notify_springboot(similarity_result)
            logger.info("ğŸš¨similarity_check -> notice spring ì „ë‹¬: {similarity_result}")
            
            return similarity_result

    except Exception as e:
        return {"error": str(e)}

def delete_file(file_path):
    try:
        print(f"ğŸ§ª ì‚­ì œ ì‹œë„: {file_path}")  # âœ… ì¶”ê°€
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")
        else:
            print(f"â“ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")  # âœ… ì¶”ê°€
    except Exception as e:
        print(f"ğŸš¨ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")

        
        
# SPRINGBOOT_URL = "http://127.0.0.1:8080/video/similarity-check"  # Spring Boot ì„œë²„ URL
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Spring Boot ì„œë²„ URL ë¡œë“œ
springboot_url = os.getenv("SPRINGBOOT_URL", "")
api_path = os.getenv("API_PATH", "")

SPRINGBOOT_URL = springboot_url + api_path

if not SPRINGBOOT_URL:
    raise ValueError("í™˜ê²½ ë³€ìˆ˜ SPRINGBOOT_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")


def notify_springboot(similarity_result):
    logger.info("ğŸš¨nofity_springboot ì§„ì…!: {similarity_result}")
    """
    Spring Boot ì„œë²„ì— ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ ì „ì†¡
    :param video_path: ê²€ì‚¬í•œ ì˜ìƒ ê²½ë¡œ
    :param similarity_result: ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼
    :param passed: ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µê³¼í–ˆëŠ”ì§€ ì—¬ë¶€ (True: í†µê³¼, False: ì‹¤íŒ¨)
    """
    payload = {
        "max_similarity": similarity_result["max_similarity"],
        "avg_similarity": similarity_result["avg_similarity"],
        "message": similarity_result["message"],
        "passed": similarity_result["passed"],
        "similar_video_id": similarity_result["similar_video_id"],
        "video_url":similarity_result["video_url"]
    }

    try:
        logger.info(f"ğŸš¨payload: {payload}")
        headers = {'Content-Type': 'application/json'}
        response = requests.post(SPRINGBOOT_URL, json=payload, headers=headers)
        logger.info(f"ğŸ“¡ Spring Boot ì‘ë‹µ: {response.status_code} - {response.text}")
    except requests.exceptions.RequestException as e:
        logger.info(f"ğŸš¨ Spring Boot ì „ì†¡ ì˜¤ë¥˜: {e}")


#ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ
def extract_frames(video_path, interval=1):
    """
    ì£¼ì–´ì§„ ì˜ìƒì—ì„œ ì¼ì • ê°„ê²©ë§ˆë‹¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    :param video_path: ì˜ìƒ íŒŒì¼ ê²½ë¡œ
    :param interval: ì´ˆ ë‹¨ìœ„ ê°„ê²© (1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì¶”ì¶œ)
    :return: í”„ë ˆì„ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise ValueError("Error: Unable to open video file.")
    
    fps = int(cap.get(cv2.CAP_PROP_FPS))  # ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜
    frame_interval = fps * interval  # ê°„ê²© ì„¤ì •
    
    frames = []
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break  # ì˜ìƒ ì¢…ë£Œ ì‹œ ë°˜ë³µ ì¤‘ì§€

        if frame_count % frame_interval == 0:
            frames.append(frame)

        frame_count += 1

    cap.release()
    return frames
    

# ResNet-50 ëª¨ë¸ ë¡œë“œ
resnet = resnet50(pretrained=True)
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±°
resnet.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜ ì •ì˜
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def extract_features(frames):
    """
    ResNet-50ì„ ì‚¬ìš©í•´ ì£¼ì–´ì§„ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ì—ì„œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œ
    :param frames: ì˜ìƒ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
    :return: íŠ¹ì§• ë²¡í„° ë¦¬ìŠ¤íŠ¸ (2048ì°¨ì›)
    """
    features = []
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for frame in frames:
            img_tensor = transform(frame).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            feature_vector = resnet(img_tensor)  # (1, 2048, 1, 1) í˜•íƒœ ì¶œë ¥
            feature_vector = feature_vector.view(-1).numpy()  # 1D ë²¡í„°ë¡œ ë³€í™˜
            features.append(feature_vector)

    return features
    

class FAISSIndex:
    def __init__(self, dim=2048):
        """
        FAISSë¥¼ ì‚¬ìš©í•´ íŠ¹ì§• ë²¡í„°ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤
        :param dim: ë²¡í„° ì°¨ì› (ResNet-50ì€ 2048)
        """
        # self.index = faiss.IndexFlatL2(dim)  # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatIP(dim) # ë‚´ì  ê¸°ë°˜ ì¸ë±ìŠ¤(ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ì í•©)

    def add_vectors(self, vectors):
        """
        ë²¡í„° ì¶”ê°€
        :param vectors: íŠ¹ì§• ë²¡í„° ë¦¬ìŠ¤íŠ¸ (numpy array)
        """
        vectors = np.array(vectors).astype('float32')
        faiss.normalize_L2(vectors) # ë²¡í„° ì •ê·œí™”
        self.index.add(vectors)

    def search(self, query_vector, top_k=5):
        """
        ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
        :param query_vector: ê²€ìƒ‰í•  ë²¡í„° (2048ì°¨ì›)
        :param top_k: ìƒìœ„ Kê°œ ê²€ìƒ‰
        :return: ìœ ì‚¬í•œ ë²¡í„° ì¸ë±ìŠ¤ì™€ ê±°ë¦¬
        """
        query_vector = np.array(query_vector).astype('float32').reshape(1, -1)
        faiss.normalize_L2(query_vector) # ì •ê·œí™”
        distances, indices = self.index.search(query_vector, top_k)
        return indices, distances
    

def convert_faiss_to_cosine():
    """L2 ê±°ë¦¬ ê¸°ë°˜ FAISSë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë³€í™˜"""
    global faiss_index

    total_vectors = faiss_index.index.ntotal
    if total_vectors == 0:
        print("ğŸ“¢ FAISSì— ì €ì¥ëœ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë³€í™˜í•  í•„ìš” ì—†ìŒ.")
        return

    print(f"ğŸ”„ ê¸°ì¡´ L2 ê¸°ë°˜ ë²¡í„° {total_vectors}ê°œ ë³€í™˜ ì¤‘...")

    # ê¸°ì¡´ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    stored_vectors = np.zeros((total_vectors, 2048), dtype=np.float32)
    for i in range(total_vectors):
        stored_vectors[i] = faiss_index.index.reconstruct(i)

    # ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
    faiss.normalize_L2(stored_vectors)

    # ìƒˆë¡œìš´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ ìƒì„± (IndexFlatIP ì‚¬ìš©)
    new_index = faiss.IndexFlatIP(2048)
    new_index.add(stored_vectors)  # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€

    # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¡œ êµì²´
    faiss_index.index = new_index

    # ë³€í™˜ëœ FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥
    save_faiss_index()

    print("âœ… L2 â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë³€í™˜ ì™„ë£Œ ë° ì €ì¥ë¨!")

 
# FAISS_INDEX_PATH = "faiss_index.bin"
FAISS_INDEX_PATH = "faiss_index/faiss_index.bin"
# âœ… faiss_index ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)


def save_faiss_index():
    """FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    faiss.write_index(faiss_index.index, FAISS_INDEX_PATH)

def load_faiss_index():
    """FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"""
    global faiss_index
    if os.path.exists(FAISS_INDEX_PATH):
        print("ğŸ”„ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        index = faiss.read_index(FAISS_INDEX_PATH)
        faiss_index.index = index  # ê¸°ì¡´ ê°ì²´ë¥¼ ë®ì–´ì”Œìš°ì§€ ì•Šê³  ìœ ì§€

# FAISS ì¸ë±ìŠ¤ ì „ì—­ ë³€ìˆ˜ë¡œ ìƒì„±
faiss_index = FAISSIndex(dim=2048)

# ì„œë²„ ì‹œì‘ ì‹œ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
load_faiss_index()
convert_faiss_to_cosine()  # âœ… ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë³€í™˜

@app.route('/process_video', methods=['POST'])
def process_video():
    data = request.json
    video_path = data.get("video_path")  # ë‹¤ìš´ë¡œë“œëœ ì˜ìƒ ê²½ë¡œ

    if not video_path or not os.path.exists(video_path):
        return jsonify({"error": "Invalid video path"}), 400

    try:
        # 1ï¸âƒ£ ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
        frames = extract_frames(video_path)

        # 2ï¸âƒ£ í”„ë ˆì„ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
        feature_vectors = extract_features(frames)

        # 3ï¸âƒ£ FAISSì— ë²¡í„° ì¶”ê°€
        faiss_index.add_vectors(feature_vectors)
        
        #4ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ì €ì¥(ë²¡í„° ì¶”ê°€ í›„ ìë™ ì €ì¥)
        save_faiss_index()

        return jsonify({"message": "Video processed successfully", "num_frames": len(frames)})

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
    
def get_next_video_id():
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            sql = "SELECT MAX(CAST(video_id AS UNSIGNED)) AS max_id FROM video_vectors"
            cursor.execute(sql)
            result = cursor.fetchone()
            conn.close()

            if result and result['max_id'] is not None:
                return str(result['max_id'] + 1)
            else:
                return "1"  # ìµœì´ˆ ì˜ìƒì¼ ê²½ìš°
    except Exception as e:
        logger.info(f"âŒ video_id ìƒì„± ì˜¤ë¥˜: {e}")
        return "1"


@app.route('/faiss_info', methods=['GET'])
def faiss_info():
    """
    í˜„ì¬ FAISSì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” API
    """
    try:
        load_faiss_index()
        total_vectors = faiss_index.index.ntotal
        logger.info(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {total_vectors}")  # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        return jsonify({"message": "FAISS index info", "total_vectors": total_vectors})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/search_similar', methods=['POST'])
def search_similar():
    data = request.json
    video_path = data.get("video_path")  # ê²€ìƒ‰í•  ì˜ìƒ íŒŒì¼ ê²½ë¡œ

    if not video_path or not os.path.exists(video_path):
        return jsonify({"error": "Invalid video path"}), 400

    try:
        # 1ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (í˜¹ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ëŒ€ë¹„)
        load_faiss_index()
        
        # 1ï¸âƒ£ ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
        frames = extract_frames(video_path)

        # 2ï¸âƒ£ í”„ë ˆì„ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
        feature_vectors = extract_features(frames)
        
        # FAISSì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜ í™•ì¸
        total_vectors = faiss_index.index.ntotal
        
        if total_vectors ==0:
            return jsonify({"error":"No vectors stored in FAISS"}), 400

        # 3ï¸âƒ£ FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰ (ì²« ë²ˆì§¸ í”„ë ˆì„ ê¸°ì¤€)
        query_vector = feature_vectors[0]
        
        top_k = min(5, total_vectors) #ì €ì¥ëœ ë²¡í„° ê°œìˆ˜ë³´ë‹¤ ë§ì§€ ì•Šê²Œ ì œí•œ
        
        indices, distances = faiss_index.search(query_vector, top_k)


        return jsonify({"message": "Search completed", "indices": indices.tolist(), "distances": distances.tolist()})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

def normalize_faiss_index():
    """FAISSì— ì €ì¥ëœ ë²¡í„°ë¥¼ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ í•¨"""
    total_vectors = faiss_index.index.ntotal
    if total_vectors > 0:
        stored_vectors = np.zeros((total_vectors, 2048), dtype=np.float32)
        for i in range(total_vectors):
            stored_vectors[i] = faiss_index.index.reconstruct(i)
        faiss.normalize_L2(stored_vectors)  # ì •ê·œí™” ìˆ˜í–‰
        faiss_index.index = faiss.IndexFlatIP(2048)  # ë‚´ì ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        faiss_index.index.add(stored_vectors)  # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€


@app.route('/check_similarity', methods=['POST'])
def check_similarity():
    """
    ì €ì¥ëœ ëª¨ë“  ë²¡í„°ì™€ ì…ë ¥ëœ ì˜ìƒì˜ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ
    - í•œ ê°œ ì´ìƒì˜ ë²¡í„°ê°€ 0.95 ì´ìƒì´ë©´ì„œ, í‰ê·  ìœ ì‚¬ë„ê°€ 0.8 ì´ìƒì´ë©´ ê°™ì€ ì˜ìƒìœ¼ë¡œ íŒë‹¨
    """
    data = request.json
    video_path = data.get("video_path")  # ë¹„êµí•  ì˜ìƒ íŒŒì¼ ê²½ë¡œ

    if not video_path or not os.path.exists(video_path):
        return jsonify({"error": "Invalid video path"}), 400

    try:
        # 1ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (íŒŒì¼ì´ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸°)
        load_faiss_index()

        # 2ï¸âƒ£ ì €ì¥ëœ ë²¡í„° ê°œìˆ˜ í™•ì¸
        total_vectors = faiss_index.index.ntotal
        if total_vectors == 0:
            return jsonify({"message": "ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µê³¼í•˜ì˜€ìŠµë‹ˆë‹¤"}), 200

        # 3ï¸âƒ£ ë¹„êµí•  ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
        frames = extract_frames(video_path)

        # 4ï¸âƒ£ í”„ë ˆì„ì˜ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
        feature_vectors = extract_features(frames)

        # 5ï¸âƒ£ ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¹„êµ)
        query_vectors = np.array(feature_vectors).astype('float32')
        faiss.normalize_L2(query_vectors)

        # 6ï¸âƒ£ ê° í”„ë ˆì„ì˜ ë²¡í„°ë¥¼ FAISSì— ì €ì¥ëœ ëª¨ë“  ë²¡í„°ì™€ ë¹„êµ
        similarity_scores = []
        for query_vector in query_vectors:
            query_vector = query_vector.reshape(1, -1)
            
            faiss.normalize_L2(query_vector) # âœ… ê²€ìƒ‰ ë²¡í„° ì •ê·œí™”
            # FAISSì—ì„œ ì €ì¥ëœ ëª¨ë“  ë²¡í„°ì™€ ìœ ì‚¬ë„ ê²€ì‚¬
            distances, _ = faiss_index.index.search(query_vector, total_vectors)

            # FAISSëŠ” ë‚´ì ì„ ë°˜í™˜í•˜ë¯€ë¡œ, ê°’ì´ í´ìˆ˜ë¡ ìœ ì‚¬í•œ ê²ƒ (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)í•¨
            similarity_scores.extend(distances[0].tolist())

        # 7ï¸âƒ£ ê²€ì‚¬ ê¸°ì¤€ ì ìš©
        max_similarity = max(similarity_scores)  # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„
        avg_similarity = sum(similarity_scores) / len(similarity_scores)  # í‰ê·  ìœ ì‚¬ë„

        if max_similarity >= 0.9 and avg_similarity >= 0.8:
            return jsonify({
                "message": "ê°™ì€ ì˜ìƒì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤",
                "max_similarity": max_similarity,
                "avg_similarity": avg_similarity
            }), 200
        else:
            return jsonify({
                "message": "ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µê³¼í•˜ì˜€ìŠµë‹ˆë‹¤",
                "max_similarity": max_similarity,
                "avg_similarity": avg_similarity
            }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
def insert_vector_metadata(video_id, start_idx, count):
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            for i in range(count):
                sql = "INSERT INTO video_vectors (video_id, faiss_index) VALUES (%s, %s)"
                cursor.execute(sql, (video_id, start_idx + i))
        conn.commit()
        conn.close()
        print(f"âœ… MySQLì— {count}ê°œ ë²¡í„° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ MySQL ì‚½ì… ì˜¤ë¥˜: {e}")


def get_video_id_by_faiss_index(faiss_index):
    logger.info("ğŸ”¥ìœ ì‚¬í•œ ì˜ìƒì„ ì°¾ìœ¼ëŸ¬ ì˜´")
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            sql = "SELECT video_id FROM video_vectors WHERE faiss_index = %s LIMIT 1"
            cursor.execute(sql, (faiss_index,))
            result = cursor.fetchone()
            conn.close()

            if result:
                logger.info(f"âœ… video_id ì°¾ìŒ: {result['video_id']}")
                return result["video_id"]
            else:
                logger.info(f"âŒ í•´ë‹¹ faiss_indexì— í•´ë‹¹í•˜ëŠ” video_id ì—†ìŒ: {faiss_index}")
                return "unknown"
    except Exception as e:
        print(f"âŒ video_id ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return "unknown"


@app.route('/test_db_insert', methods=['POST'])
def test_db_insert():
    data = request.json
    video_id = data.get("video_id")
    faiss_index = data.get("faiss_index")

    if not video_id or faiss_index is None:
        return jsonify({"error": "video_idì™€ faiss_indexëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤"}), 400

    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        # insert
        sql = "INSERT INTO video_vectors (video_id, faiss_index) VALUES (%s, %s)"
        cursor.execute(sql, (video_id, faiss_index))
        conn.commit()

        # select
        cursor.execute("SELECT * FROM video_vectors ORDER BY id DESC LIMIT 1")
        result = cursor.fetchone()

        cursor.close()
        conn.close()

        return jsonify({"message": "ì‚½ì… ì„±ê³µ", "last_inserted": result}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/health')
def health():
    return 'ok', 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
